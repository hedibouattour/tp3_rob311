# -*- coding: utf-8 -*-
"""Collector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0rpzdvdaByjGdCTOEH5Lxp_Mc2RIh-z

# Extract Data
"""

import urlparse
import requests
from urllib import urlencode

main_api = 'http://localhost:9090/api/v1/query_range?'
#time = '&start=2019-04-25T16:00:00.781Z&end=2019-05-02T03:00:00.001Z&step=5s'
#time = '&start=2019-05-15T12:49:00.781Z&end=2019-05-15T13:49:00.001Z&step=5s'     # ---> Overload
#time = '&start=2019-05-16T00:00:00.781Z&end=2019-05-16T03:00:00.001Z&step=5s'     # ---> normal 1
#time = '&start=2019-05-17T00:50:00.781Z&end=2019-05-17T05:19:00.001Z&step=5s'     # ---> normal 2
#time = '&start=2019-05-16T23:41:00.781Z&end=2019-05-17T00:11:00.001Z&step=5s'     # ---> noise 1
#time = '&start=2019-05-16T22:45:00.781Z&end=2019-05-16T23:06:00.001Z&step=5s'     # ---> noise 2
#time = '&start=2019-05-16T21:50:00.781Z&end=2019-05-16T22:11:00.001Z&step=5s'     # ---> noise 3
#time = '&start=2019-05-16T21:30:00.781Z&end=2019-05-16T22:33:00.001Z&step=5s'     # ---> noise 4
#time = '&start=2019-05-16T17:00:00.781Z&end=2019-05-16T17:04:00.001Z&step=5s'     # ---> noise 5
#time = '&start=2019-05-16T16:24:30.781Z&end=2019-05-16T16:27:30.001Z&step=5s'     # ---> noise 6
#time = '&start=2019-05-16T16:11:00.781Z&end=2019-05-16T16:23:00.001Z&step=5s'     # ---> noise 7
#time = '&start=2019-05-16T16:02:00.781Z&end=2019-05-16T16:05:00.001Z&step=5s'     # ---> noise 8
#time = '&start=2019-05-16T14:40:30.781Z&end=2019-05-16T14:44:00.001Z&step=5s'     # ---> noise 9
#time = '&start=2019-05-20T15:00:30.781Z&end=2019-05-21T03:00:00.001Z&step=5s'     # ---> normal 3

#new
time = '&start=2019-05-23T17:00:00.781Z&end=2019-05-23T018:00:00.001Z&step=5s'     # ---> TEST

query1 = '100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)'
query2 = '100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[5m]) + avg_over_time(node_memory_Cached_bytes[5m]) + avg_over_time(node_memory_Buffers_bytes[5m])) / avg_over_time(node_memory_MemTotal_bytes[5m])))'
query3 = 'rate(node_network_receive_bytes_total[5m])'
query4 = 'rate(node_network_transmit_bytes_total[5m])'

url1 = main_api + urlencode({'query': query1}) + time
url2 = main_api + urlencode({'query': query2})+ time
url3 = main_api + urlencode({'query': query3})+ time
url4 = main_api + urlencode({'query': query4})+ time

print url1
print url2
print url3
print url4

json_data1 = requests.get(url1).json()
json_data2 = requests.get(url2).json()
json_data3 = requests.get(url3).json()
json_data4 = requests.get(url4).json()
#print(json_data1)

"""# Create CSV File"""

import csv

fname = "collector.csv"
file_csv = open(fname, "w")
writer = csv.writer(file_csv)

supervised_resources_cpu = len(json_data1['data']['result'])
resources_MAP_cpu = {}
print "The number of supervised VMs and servers is: " + str (supervised_resources_cpu) 
for i in range(supervised_resources_cpu):
    print json_data1['data']['result'][i]['metric']['job']
    resources_MAP_cpu [json_data1['data']['result'][i]['metric']['job']] = i
print resources_MAP_cpu

supervised_resources_mem = len(json_data2['data']['result'])
resources_MAP_mem = {}
print "The number of supervised VMs and servers is: " + str (supervised_resources_mem) 
for i in range(supervised_resources_mem):
    print json_data2['data']['result'][i]['metric']['job']
    resources_MAP_mem [json_data2['data']['result'][i]['metric']['job']] = i
print resources_MAP_mem

supervised_network_interfaces_netin = len(json_data3['data']['result'])
network_interface_MAP_netin = {}
print "The number of supervised network interfaces is : " + str (supervised_network_interfaces_netin) 

for i in range(supervised_network_interfaces_netin):
    #print json_data3['data']['result'][i]['metric']['job']
    network_interface_MAP_netin [json_data3['data']['result'][i]['metric']['job'] + ' ' 
                           + json_data3['data']['result'][i]['metric']['device']]=i
#print network_interface_MAP
print network_interface_MAP_netin['iot-noisy-vm ens4']

#print json_data3['data']['result'][network_interface_MAP['iot-noisy-vm ens4']]['metric']
#print json_data1['data']['result'][resources_MAP['iot-noisy-vm']]['values'][10]
#print json_data2['data']['result'][resources_MAP['iot-noisy-vm']]['values'][10]
#print json_data3['data']['result'][network_interface_MAP['iot-noisy-vm ens4']]['values'][10]
#print json_data4['data']['result'][network_interface_MAP['iot-noisy-vm ens4']]['values'][10]

supervised_network_interfaces_netout = len(json_data4['data']['result'])
network_interface_MAP_netout = {}
print "The number of supervised network interfaces is : " + str (supervised_network_interfaces_netout) 

for i in range(supervised_network_interfaces_netout):
    #print json_data3['data']['result'][i]['metric']['job']
    network_interface_MAP_netout [json_data4['data']['result'][i]['metric']['job'] + ' ' 
                           + json_data4['data']['result'][i]['metric']['device']]=i
#print network_interface_MAP
print network_interface_MAP_netout['iot-noisy-vm ens4']

#writer.writerow( ('timestamp', 'cpu_usage', 'memory', 'inbound', 'outbound', 'label','class') )

for i in range (len (json_data1['data']['result'][resources_MAP_cpu['iot-noisy-vm']]['values']) ):
    writer.writerow( (json_data1['data']['result'][resources_MAP_cpu['iot-noisy-vm']]['values'][i][0],
                      json_data1['data']['result'][resources_MAP_cpu['iot-noisy-vm']]['values'][i][1], 
                      json_data2['data']['result'][resources_MAP_mem['iot-noisy-vm']]['values'][i][1], 
                      json_data3['data']['result'][network_interface_MAP_netin['iot-noisy-vm ens4']]['values'][i][1], 
                      json_data4['data']['result'][network_interface_MAP_netout['iot-noisy-vm ens4']]['values'][i][1], 'noise',1) )

file_csv.close()

"""#Concatenate csv files"""

import os 
import csv
import glob
import pandas as pd

csvfiles = glob.glob('/home/container/collector/collector*.csv')
finaldata = open('/home/container/collector/finaldata.csv', "w")
wf = csv.writer(finaldata, delimiter = ',')
#wf = csv.writer(open('/home/container/collector/finaldata.csv','w'),delimiter = ',')
wf.writerow( ('timestamp', 'cpu_usage', 'memory', 'inbound', 'outbound', 'label','class') )

for files in csvfiles:
    rd = csv.reader(open(files, 'r'), delimiter = ',')
    next(rd)
    for row in rd:
        #print (row)
        wf.writerow(row)
        
finaldata.close()

